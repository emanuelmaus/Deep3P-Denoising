{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure: 3PM-Noise2Void"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Jupyter-notebook magic\n",
    "\n",
    "# For the matplotlib \n",
    "%matplotlib inline\n",
    "# For reload functions explicitly\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "## Add the modules to the system path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "## Libs\n",
    "from random import shuffle\n",
    "import glob\n",
    "import tifffile\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.fastest = True\n",
    "\n",
    "## Own modules\n",
    "import utils\n",
    "from train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select here, if mGFP or THG should be denoised #\n",
    "denoise_mGFP = True\n",
    "#************************************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter the store path for the results and raw file here #\n",
    "path_results = os.path.join(\"..\", \"results_3PM-N2V\")\n",
    "if denoise_mGFP:\n",
    "    path_results = path_results+\"_mGFP\"\n",
    "else:\n",
    "    path_results = path_results+\"_THG\"\n",
    "\n",
    "path_dataset = os.path.join(\"..\", \"data\", \"3PM-N2V\")\n",
    "#********************************************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Output Folder*****\n",
      "List of all folder in the results path:\n",
      "['checkpoints', 'log_files', 'training_results']\n",
      "***********************\n"
     ]
    }
   ],
   "source": [
    "# Create all the other paths based on the results folder\n",
    "\n",
    "# Make a folder to store results\n",
    "res_folder = os.path.join(path_results, 'training_results')\n",
    "os.makedirs(res_folder, exist_ok=True)\n",
    "\n",
    "# Make a folder to store the log files\n",
    "log_folder = os.path.join(path_results, 'log_files')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "log_train_folder = os.path.join(log_folder, 'train')\n",
    "os.makedirs(log_train_folder, exist_ok=True)\n",
    "\n",
    "log_val_folder = os.path.join(log_folder, 'val')\n",
    "os.makedirs(log_val_folder, exist_ok=True)\n",
    "\n",
    "# Make a folder for the checkpoints\n",
    "checkpoint_folder = os.path.join(path_results, 'checkpoints')\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "# List all folders in the results folder to make sure all folder exists\n",
    "output_files = os.listdir(path_results)\n",
    "print(\"*****Output Folder*****\")\n",
    "print(\"List of all folder in the results path:\")\n",
    "print(output_files)\n",
    "print(\"***********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On following file will be trained:   ..\\data\\3PM-N2V\\RD5_3P_data-train.tif\n",
      "on following file will be validated:   ..\\data\\3PM-N2V\\RD5_3P_data-validate.tif\n"
     ]
    }
   ],
   "source": [
    "## Load image stack as dataset \n",
    "\n",
    "filenames_train = glob.glob(os.path.join(path_dataset, \"*-train.tif\"))\n",
    "filenames_val = glob.glob(os.path.join(path_dataset, \"*-validate.tif\"))\n",
    "print(\"On following file will be trained:  \", filenames_train[0])\n",
    "print(\"on following file will be validated:  \", filenames_val[0])\n",
    "\n",
    "file_train = tifffile.imread(filenames_train[0])\n",
    "file_val = tifffile.imread(filenames_val[0])\n",
    "\n",
    "if denoise_mGFP:\n",
    "    file_train = file_train[:,0].squeeze()\n",
    "    file_val = file_val[:,0].squeeze()\n",
    "else:\n",
    "    file_val = file_train[:,1].squeeze()\n",
    "    file_val = file_train[:,1].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise2Void Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the blindspot parameters #\n",
    "percent_blindpixel = 2\n",
    "picking_radius = 5\n",
    "use_N2V2_setup = True\n",
    "#*********************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the training parameters #\n",
    "# Z x Y x X\n",
    "input_size = [16, 64, 64]\n",
    "\n",
    "# #Training-to-#Validation ratio\n",
    "train_val_fraction = 0.5\n",
    "\n",
    "# Training epochs\n",
    "epoch = 300\n",
    "\n",
    "# Batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Logger frequencies\n",
    "display_freq = 500\n",
    "model_storing_freq = 50\n",
    "#*********************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameter dictionary\n",
    "parameter_dict= {}\n",
    "# paths\n",
    "# In case norm-factors are stored somewhere, not necessary\n",
    "parameter_dict['dir_norm_factors'] = os.path.join(\"no_norm_factors_stored\")\n",
    "parameter_dict['dir_checkpoint'] = checkpoint_folder\n",
    "parameter_dict['dir_log'] = log_folder\n",
    "parameter_dict['dir_result'] = res_folder\n",
    "# training state\n",
    "parameter_dict['train_continue'] = 'on'\n",
    "# hyperparameters\n",
    "parameter_dict['num_epoch'] = epoch\n",
    "# batch size\n",
    "parameter_dict['batch_size'] = batch_size\n",
    "# adam optimizer\n",
    "parameter_dict['lr'] = 0.001\n",
    "parameter_dict['optim'] = 'adam'\n",
    "parameter_dict['beta1'] = 0.5\n",
    "parameter_dict['beta2'] = 0.999\n",
    "# colormap\n",
    "parameter_dict['cmap'] = 'gray'\n",
    "# size of the input patches\n",
    "parameter_dict['ny'] = input_size[2]\n",
    "parameter_dict['nx'] = input_size[1]\n",
    "parameter_dict['nz'] = input_size[0]\n",
    "# channel dimension\n",
    "parameter_dict['nch'] = 1\n",
    "\n",
    "# augmentation data for the N2V augmenter\n",
    "parameter_dict['perc_pixel'] = percent_blindpixel\n",
    "parameter_dict['n2v_neighborhood_radius'] = picking_radius\n",
    "parameter_dict['structN2Vmask'] = None\n",
    "# Use the N2V2 setup\n",
    "parameter_dict['N2V2'] = use_N2V2_setup\n",
    "# logger parameter\n",
    "parameter_dict['num_freq_disp'] = display_freq\n",
    "parameter_dict['num_freq_save'] = model_storing_freq\n",
    "# datasets\n",
    "parameter_dict['train_dataset'] = [file_train]\n",
    "parameter_dict['val_dataset'] = [file_val[:-int(train_val_fraction*len(file_train))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Parameters *****\n",
      "{'dir_norm_factors': 'no_norm_factors_stored',\n",
      "'dir_checkpoint': '..\\\\results_3PM-N2V_mGFP\\\\checkpoints',\n",
      "'dir_log': '..\\\\results_3PM-N2V_mGFP\\\\log_files',\n",
      "'dir_result': '..\\\\results_3PM-N2V_mGFP\\\\training_results',\n",
      "'train_continue': 'on',\n",
      "'num_epoch': 300,\n",
      "'batch_size': 4,\n",
      "'lr': 0.001,\n",
      "'optim': 'adam',\n",
      "'beta1': 0.5,\n",
      "'beta2': 0.999,\n",
      "'cmap': 'gray',\n",
      "'ny': 64,\n",
      "'nx': 64,\n",
      "'nz': 16,\n",
      "'nch': 1,\n",
      "'perc_pixel': 2,\n",
      "'n2v_neighborhood_radius': 5,\n",
      "'structN2Vmask': None,\n",
      "'num_freq_disp': 500,\n",
      "'num_freq_save': 50,\n",
      "'train_dataset': [array([[[32884, 32946, 32958, ..., 32889, 32927, 32897],\n",
      "        [32985, 32886, 32890, ..., 32901, 32938, 33022],\n",
      "        [32888, 32929, 32945, ..., 32990, 33020, 33272],\n",
      "        ...,\n",
      "        [33020, 32958, 33004, ..., 33020, 33168, 32984],\n",
      "        [32931, 32930, 32892, ..., 33039, 32994, 32893],\n",
      "        [32902, 32953, 32966, ..., 32959, 32914, 32912]],\n",
      "\n",
      "       [[32905, 32888, 32934, ..., 32978, 32948, 32907],\n",
      "        [32902, 32935, 32904, ..., 33007, 32913, 32892],\n",
      "        [32936, 32896, 32906, ..., 32959, 32966, 32991],\n",
      "        ...,\n",
      "        [32974, 32890, 32899, ..., 32985, 33341, 32995],\n",
      "        [32895, 32892, 32935, ..., 32994, 32896, 32896],\n",
      "        [32886, 32919, 32953, ..., 32887, 32956, 32919]],\n",
      "\n",
      "       [[32893, 32875, 33002, ..., 32902, 32935, 32966],\n",
      "        [32914, 32898, 32890, ..., 32896, 32892, 32932],\n",
      "        [32928, 32916, 32973, ..., 32990, 32887, 32971],\n",
      "        ...,\n",
      "        [32889, 32940, 33005, ..., 32944, 32904, 32916],\n",
      "        [32891, 32922, 32921, ..., 32905, 32889, 32904],\n",
      "        [32964, 32888, 32934, ..., 32887, 32976, 32894]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[32970, 32894, 32923, ..., 32904, 32896, 32992],\n",
      "        [32960, 32960, 32921, ..., 32890, 32890, 32960],\n",
      "        [32934, 32891, 32933, ..., 32915, 32965, 32971],\n",
      "        ...,\n",
      "        [32965, 32907, 32925, ..., 32892, 32888, 32915],\n",
      "        [32895, 32892, 32910, ..., 32892, 32985, 32888],\n",
      "        [32892, 32928, 32949, ..., 32883, 32952, 32951]],\n",
      "\n",
      "       [[32886, 32886, 32876, ..., 32920, 32888, 32953],\n",
      "        [32885, 32889, 33005, ..., 32883, 32959, 32885],\n",
      "        [32903, 32930, 32900, ..., 32916, 32894, 32935],\n",
      "        ...,\n",
      "        [32895, 32913, 32910, ..., 32914, 32890, 32928],\n",
      "        [32922, 32891, 32933, ..., 32890, 32898, 32923],\n",
      "        [32918, 32912, 32896, ..., 32924, 32950, 32885]],\n",
      "\n",
      "       [[32892, 32893, 32875, ..., 32917, 32899, 32894],\n",
      "        [32905, 32885, 32902, ..., 32916, 32894, 32917],\n",
      "        [32951, 32972, 32892, ..., 32914, 32887, 32914],\n",
      "        ...,\n",
      "        [32892, 32911, 32889, ..., 32894, 32919, 32903],\n",
      "        [32913, 32916, 32904, ..., 32889, 32942, 32914],\n",
      "        [32906, 32883, 32888, ..., 32905, 32934, 33025]]], dtype=uint16)],\n",
      "'val_dataset': [array([[[32887, 32914, 33016, ..., 32971, 32881, 32890],\n",
      "        [32989, 32930, 32887, ..., 32889, 32896, 32887],\n",
      "        [32930, 32890, 32883, ..., 32989, 32884, 32992],\n",
      "        ...,\n",
      "        [32988, 32973, 32985, ..., 32891, 32971, 32899],\n",
      "        [32886, 32902, 32941, ..., 32916, 32910, 33035],\n",
      "        [32891, 32886, 32891, ..., 32887, 32896, 32907]],\n",
      "\n",
      "       [[32916, 32939, 32882, ..., 32965, 32975, 32894],\n",
      "        [32889, 32908, 32977, ..., 32906, 32888, 32970],\n",
      "        [32933, 32887, 32912, ..., 32913, 32888, 32948],\n",
      "        ...,\n",
      "        [32889, 32912, 32888, ..., 32921, 32976, 32940],\n",
      "        [32886, 32907, 32891, ..., 32954, 33013, 33015],\n",
      "        [32886, 32935, 32930, ..., 32922, 33060, 32920]],\n",
      "\n",
      "       [[32915, 32890, 32896, ..., 32888, 32891, 32887],\n",
      "        [32938, 32943, 32956, ..., 32916, 32931, 32960],\n",
      "        [32888, 32903, 32882, ..., 32888, 32861, 32890],\n",
      "        ...,\n",
      "        [32880, 32898, 32894, ..., 33071, 32940, 32888],\n",
      "        [32966, 32897, 32932, ..., 33031, 33050, 32915],\n",
      "        [32939, 32892, 32957, ..., 32942, 32921, 32880]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[32899, 32907, 32932, ..., 32896, 32890, 33056],\n",
      "        [33046, 33020, 33031, ..., 32894, 32990, 32925],\n",
      "        [33016, 33209, 33170, ..., 32905, 32911, 32960],\n",
      "        ...,\n",
      "        [32995, 32901, 32884, ..., 32964, 32907, 32891],\n",
      "        [32892, 32924, 32908, ..., 32925, 32965, 32970],\n",
      "        [32888, 32890, 32896, ..., 32961, 32925, 32922]],\n",
      "\n",
      "       [[33030, 32947, 32903, ..., 32892, 32958, 32901],\n",
      "        [32991, 32939, 33004, ..., 32892, 32905, 32885],\n",
      "        [32915, 33171, 32993, ..., 32915, 32970, 32934],\n",
      "        ...,\n",
      "        [32919, 32974, 32959, ..., 32891, 32889, 32920],\n",
      "        [32893, 33030, 32918, ..., 32889, 32900, 32944],\n",
      "        [32923, 32926, 32924, ..., 32913, 32921, 32902]],\n",
      "\n",
      "       [[32955, 33000, 32959, ..., 32921, 32893, 32885],\n",
      "        [33038, 33030, 32994, ..., 32946, 32899, 32893],\n",
      "        [32951, 32971, 32965, ..., 32897, 32934, 32909],\n",
      "        ...,\n",
      "        [32912, 32959, 33003, ..., 32912, 32907, 32915],\n",
      "        [32907, 32928, 32921, ..., 32937, 32896, 32921],\n",
      "        [32881, 32986, 32946, ..., 32916, 32898, 32922]]], dtype=uint16)],}\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# Show the parameters\n",
    "print(\"***** Parameters *****\")\n",
    "print(\"{\" + \"\\n\".join(\"{!r}: {!r},\".format(k, v) for k, v in parameter_dict.items()) + \"}\")\n",
    "print(\"**********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "TensorFlow installation not found - running with reduced feature set.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
       "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
       "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
       "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
       "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
       "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
       "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
       "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
       "                   [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--detect_file_replacement BOOL]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   [--whatif-data-dir PATH]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: argument {serve,dev}: invalid choice: 'validation:/../results_mGFP/log_files/val' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard logger\n",
    "%tensorboard --logdir train:/../results_mGFP/log_files/train validation:/../results_mGFP/log_files/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "*****Start of Training*****\n",
      "Minimum: 0.5016708628976883\t Maximum: 0.5051346608682383\n",
      "Minimum: 0.501686121919585\t Maximum: 0.5057145037003128\n",
      "Number of blindspots:     1310\n",
      "Number of blindspots:     1310\n",
      "initialize network with normal\n",
      "TRAIN: EPOCH 1: BATCH 0001/0008: LOSS1: 0.0006: LOSS2: 0.0002: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0002/0008: LOSS1: 0.0006: LOSS2: 0.0004: LOSS_TOTAL: 0.0005\n",
      "TRAIN: EPOCH 1: BATCH 0003/0008: LOSS1: 0.0006: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0004/0008: LOSS1: 0.0006: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0005/0008: LOSS1: 0.0005: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0006/0008: LOSS1: 0.0005: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0007/0008: LOSS1: 0.0005: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 1: BATCH 0008/0008: LOSS1: 0.0005: LOSS2: 0.0003: LOSS_TOTAL: 0.0004\n",
      "VALID: EPOCH 1: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 1: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 1: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 1: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 2: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 2: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 2: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 2: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 2: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 2: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 2: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 3: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 3: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 3: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 3: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 3: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 3: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 3: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 4: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 4: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 4: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 4: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 4: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 4: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 5: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 5: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 5: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 5: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 5: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 6: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 6: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 6: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 6: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 6: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 7: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 7: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 7: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 7: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 7: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 7: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 8: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 8: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 8: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 8: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 8: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 8: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 8: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 8: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 8: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 8: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 8: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 8: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 9: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 9: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 9: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 9: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 9: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 9: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 10: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 10: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 10: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 10: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 10: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 11: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 11: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 11: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 11: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 11: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 12: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 12: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 12: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 12: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 12: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 12: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 12: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0002/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0003/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 13: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 13: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 13: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 13: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 13: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 14: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 14: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 14: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 14: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 14: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 15: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 15: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 15: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 15: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 15: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 15: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 16: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 16: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 16: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 16: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 16: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 16: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 17: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 17: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 17: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 17: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 17: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 18: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 18: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 18: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 18: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 18: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 18: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 19: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 19: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 19: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 19: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 19: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 19: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 20: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 20: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 20: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 20: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 20: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 20: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 20: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 21: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 21: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 21: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 21: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 21: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 21: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 22: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 22: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 22: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 22: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 22: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 23: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 23: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 23: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 23: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 23: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 23: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 23: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 24: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 24: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 24: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 24: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 24: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 24: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 25: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 25: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 25: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 25: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 25: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 25: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 26: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 26: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 26: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 26: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 26: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 27: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 27: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 27: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 27: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 27: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 28: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 28: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 28: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 28: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 28: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 28: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 28: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 28: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 28: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 28: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 28: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 28: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 29: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 29: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 29: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 29: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 29: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 29: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 30: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 30: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 30: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 30: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 30: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 30: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 30: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 30: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 30: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 30: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 30: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 30: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 31: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 31: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 31: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 31: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 31: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 31: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 31: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 32: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 32: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 32: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 32: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 32: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 32: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 32: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 33: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 33: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 33: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 33: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 33: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 33: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 34: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 34: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 34: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 34: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 34: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 34: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 35: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 35: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 35: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 35: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 35: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 35: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 36: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 36: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 36: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 36: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 36: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 36: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 36: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 36: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 36: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 36: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 36: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 36: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 37: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 37: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 37: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 37: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 37: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 37: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 38: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 38: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 38: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 38: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 38: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 38: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 38: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 39: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 39: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 39: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 39: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 39: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 39: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 39: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 39: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 39: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 39: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 39: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 39: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 40: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 40: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 40: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 40: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 40: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 40: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 40: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 41: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 41: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 41: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 41: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 41: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 41: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 42: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 42: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 42: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 42: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 42: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 42: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 43: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 43: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 43: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 43: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 43: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 43: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 43: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 43: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 43: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 43: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 43: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 43: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 44: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 44: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 44: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 44: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 44: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 44: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 45: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 45: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 45: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 45: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 45: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 45: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 45: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 45: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 45: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 45: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 45: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 45: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 46: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0002/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 46: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 46: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 46: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 46: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 46: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 47: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 47: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 47: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 47: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 47: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 47: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 48: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 48: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 48: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 48: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 48: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 48: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 49: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 49: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 49: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 49: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 49: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 49: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 49: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 50: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 50: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 50: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 50: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 50: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 50: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 50: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 51: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 51: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 51: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 51: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 51: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 51: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 51: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 52: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0004\n",
      "TRAIN: EPOCH 52: BATCH 0002/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 52: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 52: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 52: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 52: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 52: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 53: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 53: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 53: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 53: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 53: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 53: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 53: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 53: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 53: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 53: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 53: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 53: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 54: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 54: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 54: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 54: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 54: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 55: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 55: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 55: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 55: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 55: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 55: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 56: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 56: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 56: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 56: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 56: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 56: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 56: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 56: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 56: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 56: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 56: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 56: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 57: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 57: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 57: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 57: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 57: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 57: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 57: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 58: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 58: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 58: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 58: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 58: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 58: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 59: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 59: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 59: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 59: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 59: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 59: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 60: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 60: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 60: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 60: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 60: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 60: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 61: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 61: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 61: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 61: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 61: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 62: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 62: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 62: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 62: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 62: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 62: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 63: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 63: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 63: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 63: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 63: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 63: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 64: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 64: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 64: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 64: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 64: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 64: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 64: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 64: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 64: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 64: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 64: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 64: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 65: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 65: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 65: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 65: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 65: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 65: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 65: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 65: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 65: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 65: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 65: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 65: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 66: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 66: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 66: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 66: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 66: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 66: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 66: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 66: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 66: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 66: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 66: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 66: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 67: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 67: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 67: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 67: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 67: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 68: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 68: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 68: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 68: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 68: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 68: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 68: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 68: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 68: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 68: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 68: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 68: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 69: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 69: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 69: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 69: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 69: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 69: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 69: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 70: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 70: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 70: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 70: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 70: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 70: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 71: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 71: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 71: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 71: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 71: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 71: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 72: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 72: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 72: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 72: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 72: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 72: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 73: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 73: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 73: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 73: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 73: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 73: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 73: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 73: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 73: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 73: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 73: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 73: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 74: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 74: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 74: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 74: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 74: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 74: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 74: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 74: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 74: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 74: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 74: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 74: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 75: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 75: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 75: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 75: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 75: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 75: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 75: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 76: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 76: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 76: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 76: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 76: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 76: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 76: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 76: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 76: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 76: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 76: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 76: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 77: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 77: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 77: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 77: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 77: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 77: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 77: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 77: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 77: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 77: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 77: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 77: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 78: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 78: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 78: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 78: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 78: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 78: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 78: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 78: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 78: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 78: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 78: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 78: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 79: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 79: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 79: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 79: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 79: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 79: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 80: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 80: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 80: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 80: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 80: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 80: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 81: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 81: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 81: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 81: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 81: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 82: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 82: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 82: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 82: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 82: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 82: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 82: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 82: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 82: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 82: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 82: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 82: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 83: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 83: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 83: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 83: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 83: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 83: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 84: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 84: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 84: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 84: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 84: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 85: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 85: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 85: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 85: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 85: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 85: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 85: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 86: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 86: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 86: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 86: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 86: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 86: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 86: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 86: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 86: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 86: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 86: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 86: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 87: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 87: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 87: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 87: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 87: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 87: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 88: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 88: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 88: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 88: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 88: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 88: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 89: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 89: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 89: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 89: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 89: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 89: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 90: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 90: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 90: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 90: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 90: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 90: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 90: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 90: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 90: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 90: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 90: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 90: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 91: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 91: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 91: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 91: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 91: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 91: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 92: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 92: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 92: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 92: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 92: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 92: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 93: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 93: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 93: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 93: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 93: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 93: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 93: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 93: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 93: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 93: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 93: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 93: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 94: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 94: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 94: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 94: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 94: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 94: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 95: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 95: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 95: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 95: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 95: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 96: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 96: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 96: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 96: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 96: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 96: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 97: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 97: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 97: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 97: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 97: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 97: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 97: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 98: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 98: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 98: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 98: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 98: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 99: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 99: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 99: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 99: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 99: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 99: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 99: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 99: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 99: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 99: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 99: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 99: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 100: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 100: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 100: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 100: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 100: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 100: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 101: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 101: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 101: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 101: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 101: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 101: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 101: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 101: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 101: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 101: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 101: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 101: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 102: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 102: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 102: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 102: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 102: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 102: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 103: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 103: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 103: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 103: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 103: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 103: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 104: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0002/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 104: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 104: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 104: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 104: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 104: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 105: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 105: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 105: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 105: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 105: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 105: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 105: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 105: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 105: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 105: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 105: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 105: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 106: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 106: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 106: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 106: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 106: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 106: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 106: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 106: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 106: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 106: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 106: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 106: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 107: BATCH 0001/0008: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 107: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 107: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 107: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 107: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 107: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 107: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 107: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 107: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 107: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 107: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 107: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 108: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 108: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 108: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 108: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 108: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 108: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 109: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 109: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 109: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 109: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 109: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 109: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 109: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 109: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 109: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 109: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 109: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 109: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 110: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 110: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 110: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 110: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 110: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 110: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 111: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 111: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 111: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 111: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 111: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 111: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 112: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 112: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 112: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 112: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 112: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 113: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 113: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 113: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 113: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 113: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 113: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 113: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 113: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 113: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 113: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 113: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 113: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 114: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 114: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 114: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 114: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 114: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 114: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 114: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 115: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 115: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 115: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 115: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 115: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 116: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 116: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 116: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 116: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 116: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 116: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 116: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 116: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 116: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 116: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 116: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 116: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 117: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 117: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 117: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 117: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 117: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 117: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 117: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 118: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 118: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 118: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 118: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 118: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 118: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 118: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 119: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 119: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 119: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 119: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 119: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 119: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 120: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 120: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 120: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 120: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 120: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 120: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 120: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 120: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 120: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 120: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 120: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 120: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 121: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 121: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 121: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 121: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 121: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 121: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 121: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 122: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 122: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 122: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 122: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 122: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 122: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 123: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 123: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 123: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 123: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 123: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 123: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 124: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 124: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 124: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 124: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 124: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 124: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 125: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 125: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 125: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 125: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 125: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 125: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 126: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 126: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 126: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 126: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 126: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 127: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 127: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 127: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 127: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 127: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 127: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 127: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 127: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 127: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 127: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 127: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 127: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 128: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 128: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 128: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 128: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 128: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 128: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 129: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 129: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 129: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 129: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 129: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 129: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 129: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 129: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 129: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 129: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 129: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 129: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 130: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 130: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 130: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 130: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 130: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 130: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 130: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 131: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 131: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 131: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 131: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 131: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 132: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 132: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 132: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 132: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 132: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 132: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 132: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 132: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 132: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 132: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 132: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 132: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 133: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 133: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 133: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 133: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 133: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 133: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 133: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 134: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 134: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 134: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 134: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 134: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 134: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 135: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 135: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 135: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 135: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 135: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 135: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 135: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 135: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 135: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 135: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 135: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 135: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 136: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 136: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 136: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 136: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 136: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 136: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 137: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 137: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 137: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 137: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 137: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 137: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 138: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 138: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 138: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 138: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 138: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 138: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 138: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 139: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 139: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 139: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 139: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 139: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 139: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 139: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 140: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 140: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 140: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 140: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 140: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 140: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 140: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 140: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 140: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 140: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 140: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 140: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 141: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 141: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 141: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 141: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 141: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 142: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 142: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 142: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 142: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 142: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 142: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 142: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 143: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 143: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 143: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 143: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 143: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 143: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 143: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 144: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 144: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 144: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 144: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 144: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 144: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 144: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 144: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 144: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 144: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 144: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 144: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 145: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 145: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 145: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 145: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 145: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 145: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 145: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 145: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 145: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 145: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 145: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 145: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 146: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 146: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 146: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 146: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 146: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 146: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 146: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 146: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 146: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 146: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 146: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 146: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 147: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 147: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 147: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 147: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 147: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 147: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 147: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 147: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 147: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 147: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 147: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 147: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 148: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 148: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 148: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 148: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 148: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 148: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 148: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 149: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 149: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 149: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 149: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 149: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 149: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 150: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 150: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 150: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 150: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 150: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 150: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 150: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 150: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 150: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 150: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 150: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 150: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0001/0008: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 151: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 151: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 151: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 151: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 151: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 151: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 152: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 152: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 152: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 152: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 152: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 153: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 153: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 153: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 153: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 153: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 153: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 153: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 153: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 153: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 153: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 153: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 153: BATCH 0004/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 154: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 154: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 154: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 154: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 154: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 154: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 154: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 154: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 154: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 154: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 154: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 154: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 155: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 155: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 155: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 155: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 155: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 155: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 156: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 156: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 156: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 156: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 156: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 156: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 156: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 157: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 157: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 157: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 157: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 157: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 157: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 157: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 157: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 157: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 157: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 157: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 157: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 158: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 158: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 158: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 158: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 158: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 158: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 158: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 159: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 159: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 159: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 159: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 159: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 159: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 160: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 160: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 160: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 160: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 160: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 160: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 161: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 161: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 161: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 161: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 161: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 161: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 161: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 162: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 162: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 162: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 162: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 162: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 162: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 162: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 162: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 162: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 162: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 162: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 162: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 163: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 163: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 163: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 163: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 163: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 163: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 164: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 164: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 164: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 164: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 164: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 164: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 165: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 165: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 165: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 165: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 165: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 165: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 165: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 166: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 166: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 166: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 166: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 166: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 166: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 166: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 166: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 166: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 166: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 166: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 166: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 167: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 167: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 167: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 167: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 167: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 167: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 167: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 167: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 167: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 167: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 167: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 167: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 168: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 168: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 168: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 168: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 168: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 168: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 168: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 168: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 168: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 168: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 168: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 168: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 169: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 169: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 169: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 169: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 169: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 169: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 170: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 170: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 170: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 170: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 170: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 170: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 170: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 170: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 170: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 170: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 170: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 170: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 171: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 171: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 171: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 171: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 171: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 171: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 171: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 171: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 171: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 171: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 171: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 171: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 172: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 172: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 172: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 172: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 172: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 172: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 172: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 172: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 172: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 172: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 172: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 172: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 173: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 173: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 173: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 173: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 173: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 173: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 174: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 174: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 174: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 174: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 174: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 174: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 175: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 175: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 175: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 175: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 175: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 175: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 176: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 176: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 176: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 176: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 176: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 176: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 176: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 176: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 176: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 176: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 176: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 176: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 177: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 177: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 177: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 177: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 177: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 177: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 177: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 177: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 177: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 177: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 177: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 177: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 178: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 178: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 178: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 178: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 178: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 178: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 179: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 179: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 179: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 179: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 179: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 179: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 179: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 179: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 179: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 179: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 179: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 179: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 180: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 180: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 180: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 180: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 180: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 180: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 181: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 181: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 181: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 181: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 181: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 181: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 181: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 182: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 182: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 182: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 182: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 182: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 182: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 183: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 183: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 183: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 183: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 183: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 183: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 183: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 184: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 184: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 184: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 184: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 184: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 184: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 185: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 185: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 185: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 185: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 185: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 185: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 185: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 185: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 185: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 185: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 185: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 185: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 186: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0002/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 186: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 186: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 186: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 186: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 186: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 187: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 187: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 187: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 187: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 187: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 187: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 187: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 188: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 188: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 188: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 188: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 188: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 188: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 189: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 189: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 189: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 189: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 189: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 189: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 190: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 190: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 190: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 190: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 190: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 190: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 190: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 190: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 190: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 190: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 190: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 190: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 191: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 191: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 191: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 191: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 191: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 191: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 191: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 192: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 192: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 192: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 192: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 192: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 192: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 193: BATCH 0001/0008: LOSS1: 0.0005: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 193: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 193: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 193: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 193: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 193: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 194: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 194: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 194: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 194: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 194: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 194: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 194: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 194: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 194: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 194: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 194: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 194: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 195: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 195: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 195: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 195: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 195: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 195: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 196: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 196: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 196: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 196: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 196: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 196: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 196: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 197: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 197: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 197: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 197: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 197: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 197: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 197: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 197: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 197: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 197: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 197: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 197: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 198: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 198: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 198: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 198: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 198: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 198: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 198: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 198: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 198: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 198: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 198: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 198: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 199: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 199: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 199: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 199: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 199: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 199: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 199: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 199: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 199: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 199: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 199: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 199: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 200: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 200: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 200: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 200: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 200: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 200: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 200: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 201: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 201: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 201: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 201: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 201: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 201: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 201: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 201: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 201: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 201: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 201: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 201: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 202: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 202: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 202: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 202: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 202: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 202: BATCH 0004/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 203: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 203: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 203: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 203: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 203: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 203: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 204: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 204: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 204: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 204: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 204: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 204: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 204: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 205: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 205: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 205: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 205: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 205: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 205: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 205: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 205: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 205: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 205: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 205: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 205: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 206: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 206: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 206: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 206: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 206: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 206: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 206: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 207: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 207: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 207: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 207: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 207: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 207: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 207: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 208: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 208: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 208: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 208: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 208: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 208: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 209: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 209: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 209: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 209: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 209: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 209: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 210: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 210: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 210: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 210: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 210: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 210: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 210: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 210: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 210: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 210: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 210: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 210: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 211: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 211: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 211: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 211: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 211: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 211: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 211: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 212: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 212: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 212: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 212: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 212: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 213: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 213: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 213: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 213: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 213: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 213: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 213: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 213: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 213: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 213: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 213: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 213: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 214: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 214: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 214: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 214: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 214: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 214: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 215: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 215: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 215: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 215: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 215: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 215: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 216: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 216: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 216: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 216: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 216: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 216: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 216: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 217: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 217: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 217: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 217: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 217: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 217: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 218: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 218: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 218: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 218: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 218: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 218: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 219: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 219: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 219: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 219: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 219: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 219: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 219: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 219: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 219: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 219: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 219: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 219: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 220: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 220: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 220: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 220: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 220: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 221: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 221: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 221: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 221: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 221: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 221: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 222: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 222: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 222: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 222: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 222: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 222: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 222: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 222: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 222: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 222: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 222: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 222: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 223: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 223: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 223: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 223: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 223: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 223: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 223: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 224: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 224: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 224: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 224: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 224: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 225: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 225: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 225: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 225: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 225: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 225: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 226: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 226: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 226: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 226: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 226: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 226: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 227: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 227: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 227: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 227: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 227: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 227: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 227: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 227: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 227: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 227: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 227: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 227: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 228: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 228: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 228: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 228: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 228: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 228: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 228: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 228: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 228: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 228: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 228: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 228: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 229: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 229: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 229: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 229: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 229: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 229: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 229: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 229: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 229: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 229: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 229: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 229: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 230: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 230: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 230: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 230: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 230: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 230: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 231: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 231: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 231: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 231: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 231: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 231: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 231: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 231: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 231: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 231: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 231: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 231: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 232: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 232: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 232: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 232: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 232: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 232: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 233: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 233: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 233: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 233: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 233: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 233: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 233: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 233: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 233: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 233: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 233: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 233: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 234: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 234: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 234: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 234: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 234: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 234: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 234: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 235: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 235: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 235: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 235: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 235: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 235: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 235: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 236: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 236: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 236: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 236: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 236: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 236: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 236: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 237: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 237: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 237: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 237: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 237: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 237: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 237: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 237: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 237: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 237: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 237: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 237: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 238: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 238: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 238: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 238: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 238: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 238: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 239: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 239: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 239: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 239: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 239: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 239: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 239: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 239: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 239: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 239: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 239: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 239: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 240: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 240: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 240: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 240: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 240: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 240: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 240: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 241: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 241: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 241: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 241: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 241: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 241: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 241: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 242: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 242: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 242: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 242: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 242: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 242: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 242: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 242: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 242: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 242: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 242: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 242: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 243: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 243: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 243: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 243: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 243: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 243: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 243: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 243: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 243: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 243: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 243: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 243: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 244: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 244: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 244: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 244: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 244: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 244: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 244: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 244: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 244: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 244: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 244: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 244: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 245: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 245: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 245: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 245: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 245: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 246: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 246: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 246: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 246: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 246: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 246: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 247: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 247: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 247: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 247: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 247: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 247: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 247: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 248: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 248: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 248: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 248: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 248: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 248: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 249: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 249: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 249: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 249: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 249: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 249: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 249: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 250: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 250: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 250: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 250: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 250: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 250: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 250: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 250: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 250: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 250: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 250: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 250: BATCH 0004/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 251: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 251: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 251: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 251: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 251: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 251: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 251: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 252: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 252: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 252: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 252: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 252: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 252: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 252: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 252: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 252: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 252: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 252: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 252: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 253: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 253: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 253: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 253: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 253: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 253: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 253: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 254: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 254: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 254: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 254: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 254: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 254: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 255: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 255: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 255: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 255: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 255: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 255: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 255: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 256: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 256: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 256: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 256: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 256: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 256: BATCH 0004/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 257: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 257: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 257: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 257: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 257: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 258: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 258: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 258: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 258: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 258: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 258: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 258: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 259: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 259: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 259: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 259: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 259: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 259: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 259: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 259: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 259: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 259: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 259: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 259: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 260: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 260: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 260: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 260: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 260: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 260: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 260: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 261: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 261: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 261: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 261: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 261: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 261: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 261: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 261: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 261: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 261: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 261: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 261: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 262: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 262: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 262: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 262: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 262: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 262: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 263: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 263: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 263: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 263: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 263: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 263: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 263: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 263: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 263: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 263: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 263: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 263: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 264: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 264: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 264: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 264: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 264: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 264: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 265: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 265: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 265: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 265: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 265: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 265: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 266: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 266: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 266: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 266: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 266: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 266: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 266: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 267: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 267: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 267: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 267: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 267: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 267: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 268: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 268: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 268: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 268: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 268: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 268: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 268: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 269: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 269: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 269: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 269: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 269: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 270: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 270: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 270: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 270: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 270: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 270: BATCH 0003/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 270: BATCH 0004/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 271: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 271: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 271: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 271: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 271: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 271: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 272: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 272: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 272: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 272: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 272: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 273: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 273: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 273: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 273: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 273: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 273: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 274: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 274: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 274: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 274: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 274: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 275: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0002: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 275: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 275: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 275: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 275: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 275: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 276: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 276: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 276: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 276: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 276: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 276: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 276: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 277: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 277: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 277: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 277: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 277: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 277: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 277: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 278: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 278: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 278: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 278: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 278: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 278: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 278: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 279: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 279: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 279: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 279: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 279: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 279: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 279: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 279: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 279: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 279: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 279: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 279: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 280: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 280: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 280: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 280: BATCH 0002/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 280: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 280: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 281: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 281: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 281: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 281: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 281: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 281: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 282: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 282: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 282: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 282: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 282: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 282: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 282: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 283: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 283: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 283: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 283: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 283: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 283: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 283: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 283: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 283: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 283: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 283: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 283: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 284: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 284: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 284: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 284: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 284: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 284: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 284: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 284: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 284: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 284: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 284: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 284: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 285: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 285: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 285: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 285: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 285: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 285: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 285: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 286: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 286: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 286: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 286: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 286: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 286: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 287: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 287: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 287: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 287: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 287: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 287: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 288: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 288: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 288: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 288: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 288: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 288: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 288: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 288: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 288: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 288: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 288: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 288: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 289: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 289: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 289: BATCH 0001/0004: LOSS1: 0.0003: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 289: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 289: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 289: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 290: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 290: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 290: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 290: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 290: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 290: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 291: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 291: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 291: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 291: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 291: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 291: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 291: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 292: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 292: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 292: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 292: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 292: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 292: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 292: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 293: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 293: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 293: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 293: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 293: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 293: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 293: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 293: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 293: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 293: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 293: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 293: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 294: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 294: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 294: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 294: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 294: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 294: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 295: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 295: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 295: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 295: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 295: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 295: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 295: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 295: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 295: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 295: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 295: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 295: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 296: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 296: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 296: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 296: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 296: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 296: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 297: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 297: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 297: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 297: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 297: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 297: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 298: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 298: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 298: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 298: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 298: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 298: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 299: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 299: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 299: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "VALID: EPOCH 299: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 299: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 299: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 299: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0001/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0002/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0003/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0004/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0005/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0006/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "TRAIN: EPOCH 300: BATCH 0007/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0003\n",
      "TRAIN: EPOCH 300: BATCH 0008/0008: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 300: BATCH 0001/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 300: BATCH 0002/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 300: BATCH 0003/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "VALID: EPOCH 300: BATCH 0004/0004: LOSS1: 0.0004: LOSS2: 0.0001: LOSS_TOTAL: 0.0002\n",
      "*****End of Training*******\n"
     ]
    }
   ],
   "source": [
    "# Generate Trainer\n",
    "trainer = Trainer(parameter_dict)\n",
    "# Start training\n",
    "print(\"*****Start of Training*****\")\n",
    "trainer.train()\n",
    "print(\"*****End of Training*******\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
