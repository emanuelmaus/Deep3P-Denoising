{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference procedure: 3PM-Noise2Void"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter-notebook magic\n",
    "\n",
    "# For the matplotlib \n",
    "%matplotlib inline\n",
    "# For reload functions explicitly\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "## Add the modules to the system path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "## Libs\n",
    "import numpy as np\n",
    "import glob\n",
    "import tifffile\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.fastest = True\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Own modules\n",
    "import utils\n",
    "from transformations import PercentileNormalize3D, PercentileDenormalize3D, ZCrop3D, ToTensor3D, ToNumpy3D\n",
    "from dataset import InferenceDataset3D\n",
    "from network import Noise2NoiseUNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select here, if mGFP or THG should be denoised #\n",
    "denoise_mGFP = True\n",
    "#************************************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the store path for the results and raw file here #\n",
    "path_results = os.path.join(\"..\", \"results\")\n",
    "if denoise_mGFP:\n",
    "    path_results = path_results+\"_mGFP\"\n",
    "else:\n",
    "    path_results = path_results+\"_THG\"\n",
    "\n",
    "path_dataset = os.path.join(\"..\", \"data\", \"3PM-N2V\")\n",
    "#********************************************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the inference based on the results folder\n",
    "\n",
    "# Make a folder to store the inference\n",
    "inference_folder = os.path.join(path_results, 'inference_results')\n",
    "os.makedirs(inference_folder, exist_ok=True)\n",
    "\n",
    "# Define path to the checkpoint folder\n",
    "checkpoint_folder = os.path.join(path_results, 'checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load image stack for inference\n",
    "filenames = glob.glob(os.path.join(path_dataset, \"*-train.tif\"))\n",
    "print(\"Following file will be denoised:  \", filenames[0])\n",
    "\n",
    "file = tifffile.imread(filenames[0])\n",
    "\n",
    "if denoise_mGFP:\n",
    "    file = file[:,0].squeeze()\n",
    "else:\n",
    "    file = file[:,1].squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the inference parameters #\n",
    "\n",
    "# Z size of the input substack (should minimum the same as in training)\n",
    "z_input_size = 16\n",
    "# Shows the overlap\n",
    "stride_to_size_ratio=2\n",
    "# Crop top and bottom z-slices to prevent artefacts\n",
    "z_crop_width = (2, 2)\n",
    "\n",
    "#********************************#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and function needed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatype of the original data\n",
    "data_type = file.dtype\n",
    "print(\"The data type of the raw data is:   \", data_type)\n",
    "\n",
    "# calculate the norm. factors\n",
    "print(\"The norm. factors are: \")\n",
    "min_img, max_img = utils.calc_normfactors(file)\n",
    "\n",
    "# check if GPU is accessable\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU will be used.\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"\\nCPU will be used.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling\n",
    "# Use the right back-conversation\n",
    "if data_type == np.uint16:\n",
    "    norm_func = utils.NormFloat2UInt16(percent=1.0)\n",
    "elif data_type == np.int16:\n",
    "    norm_func = utils.NormFloat2Int16(percent=1.0)\n",
    "else:\n",
    "    norm_func = utils.NormFloat2UInt8(percent=1.0)\n",
    "\n",
    "## Transformation\n",
    "transform = transforms.Compose([PercentileNormalize3D(mi=min_img, ma=max_img),\n",
    "                                ToTensor3D()\n",
    "                                ])\n",
    "transform_inv = transforms.Compose([ToNumpy3D(),\n",
    "                                    ZCrop3D(z_crop_width),\n",
    "                                    PercentileDenormalize3D(mi=min_img, ma=max_img),\n",
    "                                    norm_func\n",
    "                                   ])\n",
    "## Inference Dataset and Dataloader\n",
    "inference_dataset = InferenceDataset3D(file, z_crop_width, z_input_size, stride_to_size_ratio, transform=transform)\n",
    "inference_loader = torch.utils.data.DataLoader(inference_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the inference data and for sanity check the input data\n",
    "size_prediction = inference_dataset.get_output_size()\n",
    "\n",
    "input_data = np.empty(size_prediction, dtype=np.float64)\n",
    "inference_data = np.empty(size_prediction, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pretrained model\n",
    "net = Noise2NoiseUNet3D(in_channels = 1,\n",
    "                        out_channels = 1,\n",
    "                        final_sigmoid = True).to(device)\n",
    "\n",
    "net, st_epoch = utils.load(checkpoint_folder, net, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the model\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "\n",
    "    print(f'Number of batches: {len(inference_folder)}')\n",
    "\n",
    "    # Going through whole dataset\n",
    "    for batch, data in enumerate(tqdm(inference_loader)):\n",
    "\n",
    "        input, current_index, norm_factors = data\n",
    "        cropped_current_index = current_index.squeeze() \n",
    "        cropped_norm_factors = norm_factors.squeeze() \n",
    "        \n",
    "        input = input.to(device)\n",
    "        # forward net\n",
    "        output = net(input)\n",
    "        ## transform data back\n",
    "        inference_data[cropped_current_index] += cropped_norm_factors.numpy()[..., None, None] * transform_inv(output).squeeze()\n",
    "        input_data[cropped_current_index] += cropped_norm_factors.numpy()[..., None, None] * transform_inv(input).squeeze()\n",
    "\n",
    "# Crop the data to its original size\n",
    "z_begin, z_end = inference_dataset.get_cropping_indices()\n",
    "inference_data = inference_data[z_begin:z_end]\n",
    "input_data = input_data[z_begin:z_end]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "ind = file.shape[0]//2\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title(\"Raw (Original)\")\n",
    "plt.imshow(file[ind], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Raw (Input Data)\")\n",
    "plt.imshow(input_data[ind], cmap=\"gray\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title(\"Denoised\")\n",
    "plt.imshow(inference_data[ind], cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_inference = os.path.join(inference_folder, filenames[0].split('/')[-1][:-4] + '-denoised.tif')\n",
    "tifffile.imwrite(store_name_inference, np.rint(inference_data).astype(data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name_input = os.path.join(inference_folder, filenames[0].split('/')[-1][:-4] + '-input.tif')\n",
    "tifffile.imwrite(store_name_input, np.rint(inference_data).astype(data_type))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
